---
title: "Position Embedding"
---

## 1. 为什么需要位置编码？

Transformer架构中的Self-Attention机制本质上是**置换不变的(permutation invariant)**。给定一个序列，无论token的顺序如何打乱，Attention的计算结果都是相同的（只是顺序不同）。

考虑输入序列 $X = [x_1, x_2, ..., x_n]$，Self-Attention的计算：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

这个计算过程中，$x_i$ 和 $x_j$ 之间的关系只取决于它们的内容，而与位置无关。但在自然语言中，"我爱你"和"你爱我"的语义完全不同，因此我们需要**位置编码(Position Embedding)**来引入位置信息。

---

## 2. Sinusoidal Position Embedding (Sin/Cos位置编码)

### 2.1 基本思想

Transformer原论文提出使用正弦和余弦函数来编码位置：

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

其中：
- $pos$：token在序列中的位置（0, 1, 2, ...）
- $i$：维度索引（0, 1, 2, ..., $d_{model}/2 - 1$）
- $d_{model}$：模型的隐藏维度

### 2.2 详细计算过程与结果

设 $d_{model} = 8$，则维度索引 $i \in \{0, 1, 2, 3\}$。

#### 2.2.1 频率计算

每个维度对的频率为：

$$
\omega_i = \frac{1}{10000^{2i/d_{model}}}
$$

| $i$ | $\omega_i = 10000^{-2i/8}$ | 数值 |
|-----|---------------------------|------|
| 0 | $10000^{0} = 1$ | 1.0 |
| 1 | $10000^{-0.25} = 10^{-1}$ | 0.1 |
| 2 | $10000^{-0.5} = 10^{-2}$ | 0.01 |
| 3 | $10000^{-0.75} = 10^{-3}$ | 0.001 |

#### 2.2.2 $\sin(pos \cdot \omega_i)$ 计算结果

| pos \ i | $i=0$ ($\omega=1$) | $i=1$ ($\omega=0.1$) | $i=2$ ($\omega=0.01$) | $i=3$ ($\omega=0.001$) |
|---------|-------------------|---------------------|----------------------|------------------------|
| 0 | $\sin(0)=0$ | $\sin(0)=0$ | $\sin(0)=0$ | $\sin(0)=0$ |
| 1 | $\sin(1)=0.841$ | $\sin(0.1)=0.0998$ | $\sin(0.01)=0.0100$ | $\sin(0.001)=0.0010$ |
| 2 | $\sin(2)=0.909$ | $\sin(0.2)=0.1987$ | $\sin(0.02)=0.0200$ | $\sin(0.002)=0.0020$ |
| 3 | $\sin(3)=0.141$ | $\sin(0.3)=0.2955$ | $\sin(0.03)=0.0300$ | $\sin(0.003)=0.0030$ |
| 4 | $\sin(4)=-0.757$ | $\sin(0.4)=0.3894$ | $\sin(0.04)=0.0400$ | $\sin(0.004)=0.0040$ |
| 5 | $\sin(5)=-0.959$ | $\sin(0.5)=0.4794$ | $\sin(0.05)=0.0500$ | $\sin(0.005)=0.0050$ |
| 6 | $\sin(6)=-0.279$ | $\sin(0.6)=0.5646$ | $\sin(0.06)=0.0600$ | $\sin(0.006)=0.0060$ |
| 7 | $\sin(7)=0.657$ | $\sin(0.7)=0.6442$ | $\sin(0.07)=0.0699$ | $\sin(0.007)=0.0070$ |
| 8 | $\sin(8)=0.989$ | $\sin(0.8)=0.7174$ | $\sin(0.08)=0.0799$ | $\sin(0.008)=0.0080$ |
| 9 | $\sin(9)=0.412$ | $\sin(0.9)=0.7833$ | $\sin(0.09)=0.0899$ | $\sin(0.009)=0.0090$ |
| 10 | $\sin(10)=-0.544$ | $\sin(1.0)=0.8415$ | $\sin(0.1)=0.0998$ | $\sin(0.01)=0.0100$ |

#### 2.2.3 $\cos(pos \cdot \omega_i)$ 计算结果

| pos \ i | $i=0$ ($\omega=1$) | $i=1$ ($\omega=0.1$) | $i=2$ ($\omega=0.01$) | $i=3$ ($\omega=0.001$) |
|---------|-------------------|---------------------|----------------------|------------------------|
| 0 | $\cos(0)=1$ | $\cos(0)=1$ | $\cos(0)=1$ | $\cos(0)=1$ |
| 1 | $\cos(1)=0.540$ | $\cos(0.1)=0.9950$ | $\cos(0.01)=0.99995$ | $\cos(0.001)=1.0000$ |
| 2 | $\cos(2)=-0.416$ | $\cos(0.2)=0.9801$ | $\cos(0.02)=0.9998$ | $\cos(0.002)=1.0000$ |
| 3 | $\cos(3)=-0.990$ | $\cos(0.3)=0.9553$ | $\cos(0.03)=0.9996$ | $\cos(0.003)=1.0000$ |
| 4 | $\cos(4)=-0.654$ | $\cos(0.4)=0.9211$ | $\cos(0.04)=0.9992$ | $\cos(0.004)=1.0000$ |
| 5 | $\cos(5)=0.284$ | $\cos(0.5)=0.8776$ | $\cos(0.05)=0.9988$ | $\cos(0.005)=1.0000$ |
| 6 | $\cos(6)=0.960$ | $\cos(0.6)=0.8253$ | $\cos(0.06)=0.9982$ | $\cos(0.006)=1.0000$ |
| 7 | $\cos(7)=0.754$ | $\cos(0.7)=0.7648$ | $\cos(0.07)=0.9976$ | $\cos(0.007)=1.0000$ |
| 8 | $\cos(8)=-0.146$ | $\cos(0.8)=0.6967$ | $\cos(0.08)=0.9968$ | $\cos(0.008)=1.0000$ |
| 9 | $\cos(9)=-0.911$ | $\cos(0.9)=0.6216$ | $\cos(0.09)=0.9960$ | $\cos(0.009)=1.0000$ |
| 10 | $\cos(10)=-0.839$ | $\cos(1.0)=0.5403$ | $\cos(0.1)=0.9950$ | $\cos(0.01)=0.99995$ |

#### 2.2.4 完整PE向量

将sin和cos交替排列，得到每个位置的完整PE向量（$d_{model}=8$）：

$$
PE_{pos} = [\sin(pos \cdot \omega_0), \cos(pos \cdot \omega_0), \sin(pos \cdot \omega_1), \cos(pos \cdot \omega_1), ...]
$$

| pos | $PE_{pos}$ |
|-----|-----------|
| 0 | $[0, 1, 0, 1, 0, 1, 0, 1]$ |
| 1 | $[0.841, 0.540, 0.100, 0.995, 0.010, 1.000, 0.001, 1.000]$ |
| 2 | $[0.909, -0.416, 0.199, 0.980, 0.020, 1.000, 0.002, 1.000]$ |
| 3 | $[0.141, -0.990, 0.296, 0.955, 0.030, 1.000, 0.003, 1.000]$ |
| 4 | $[-0.757, -0.654, 0.389, 0.921, 0.040, 0.999, 0.004, 1.000]$ |
| 5 | $[-0.959, 0.284, 0.479, 0.878, 0.050, 0.999, 0.005, 1.000]$ |

#### 2.2.5 规律总结

1. **低维度（$i$小）**：频率高，随pos快速振荡，周期短（$T_0 = 2\pi \approx 6.28$）
2. **高维度（$i$大）**：频率低，随pos缓慢变化，周期长（$T_3 = 2\pi \times 1000 \approx 6283$）
3. **不同pos的区分度**：低维提供细粒度区分，高维提供粗粒度区分
4. **类似进制编码**：低维如"秒针"快速转动，高维如"时针"缓慢转动

### 2.3 为什么选择Sin/Cos？

**关键性质：相对位置可以通过线性变换表示**

对于任意固定的偏移量 $k$，存在一个线性变换矩阵 $M$，使得：

$$
PE_{pos+k} = M \cdot PE_{pos}
$$

证明：利用三角函数的和角公式：

$$
\sin(pos + k) = \sin(pos)\cos(k) + \cos(pos)\sin(k)
$$

$$
\cos(pos + k) = \cos(pos)\cos(k) - \sin(pos)\sin(k)
$$

写成矩阵形式：

$$
\begin{bmatrix} \sin(pos + k) \\ \cos(pos + k) \end{bmatrix} = \begin{bmatrix} \cos(k) & \sin(k) \\ -\sin(k) & \cos(k) \end{bmatrix} \begin{bmatrix} \sin(pos) \\ \cos(pos) \end{bmatrix}
$$

这意味着模型可以学习到相对位置关系！

### 2.4 Sin/Cos位置编码的局限性

1. **加法方式混合**：位置信息通过 $x + PE$ 加入，容易与语义信息混淆
2. **外推能力有限**：虽然理论上可以外推，但实际效果有限
3. **绝对位置编码**：本质上还是编码绝对位置

---

## 3. RoPE (Rotary Position Embedding)

### 3.1 核心思想

RoPE的核心思想是：**通过旋转矩阵将位置信息编码到Attention的Query和Key中，使得内积只依赖于相对位置**。

目标：设计一个位置编码函数 $f(x, pos)$，使得：

$$
\langle f(q, m), f(k, n) \rangle = g(q, k, m-n)
$$

即Query和Key的内积只与它们的**相对位置** $m-n$ 有关。

### 3.2 数学推导

#### 二维情况

从最简单的二维情况开始。设 $q = [q_0, q_1]^T$，我们希望找到函数 $f$。

**旋转矩阵**：将向量 $q$ 旋转角度 $\theta$：

$$
R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
$$

定义位置编码函数：

$$
f(q, m) = R(m\theta) \cdot q = \begin{bmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{bmatrix} \begin{bmatrix} q_0 \\ q_1 \end{bmatrix}
$$

展开计算：

$$
f(q, m) = \begin{bmatrix} q_0\cos(m\theta) - q_1\sin(m\theta) \\ q_0\sin(m\theta) + q_1\cos(m\theta) \end{bmatrix}
$$

#### 验证相对位置性质

计算 $f(q, m)$ 和 $f(k, n)$ 的内积：

$$
\langle f(q, m), f(k, n) \rangle = f(q, m)^T \cdot f(k, n)
$$

利用旋转矩阵的性质 $R(\alpha)^T = R(-\alpha)$：

$$
= q^T R(m\theta)^T R(n\theta) k = q^T R(-m\theta) R(n\theta) k = q^T R((n-m)\theta) k
$$

结果只依赖于 $(n-m)$，即**相对位置**！

### 3.3 高维扩展

对于 $d$ 维向量（$d$ 为偶数），将其分成 $d/2$ 个二维子空间，每个子空间独立旋转：

$$
R_{\Theta, m} = \begin{bmatrix}
\cos(m\theta_0) & -\sin(m\theta_0) & 0 & 0 & \cdots \\
\sin(m\theta_0) & \cos(m\theta_0) & 0 & 0 & \cdots \\
0 & 0 & \cos(m\theta_1) & -\sin(m\theta_1) & \cdots \\
0 & 0 & \sin(m\theta_1) & \cos(m\theta_1) & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}
$$

其中频率设定与Sinusoidal类似：

$$
\theta_i = 10000^{-2i/d}
$$

### 3.4 详细计算示例

假设 $d = 4$，位置 $m = 2$，输入向量 $q = [1, 2, 3, 4]^T$

**Step 1：计算各频率**

$$
\theta_0 = 10000^{-0/4} = 1
$$

$$
\theta_1 = 10000^{-2/4} = 10000^{-0.5} = 0.01
$$

**Step 2：计算旋转角度**

$$
m\theta_0 = 2 \times 1 = 2
$$

$$
m\theta_1 = 2 \times 0.01 = 0.02
$$

**Step 3：构建旋转矩阵**

$$
R_{\Theta, 2} = \begin{bmatrix}
\cos(2) & -\sin(2) & 0 & 0 \\
\sin(2) & \cos(2) & 0 & 0 \\
0 & 0 & \cos(0.02) & -\sin(0.02) \\
0 & 0 & \sin(0.02) & \cos(0.02)
\end{bmatrix}
$$

数值代入（$\cos(2) \approx -0.416$，$\sin(2) \approx 0.909$，$\cos(0.02) \approx 1.0$，$\sin(0.02) \approx 0.02$）：

$$
R_{\Theta, 2} \approx \begin{bmatrix}
-0.416 & -0.909 & 0 & 0 \\
0.909 & -0.416 & 0 & 0 \\
0 & 0 & 1.0 & -0.02 \\
0 & 0 & 0.02 & 1.0
\end{bmatrix}
$$

**Step 4：应用旋转**

$$
f(q, 2) = R_{\Theta, 2} \cdot q = \begin{bmatrix}
-0.416 \times 1 + (-0.909) \times 2 \\
0.909 \times 1 + (-0.416) \times 2 \\
1.0 \times 3 + (-0.02) \times 4 \\
0.02 \times 3 + 1.0 \times 4
\end{bmatrix} = \begin{bmatrix}
-2.234 \\
0.077 \\
2.92 \\
4.06
\end{bmatrix}
$$

### 3.5 高效实现

直接构建完整旋转矩阵计算量大，实际实现中使用等价的逐元素运算：

$$
f(x, m) = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ x_3 \\ \vdots \end{bmatrix} \odot \begin{bmatrix} \cos(m\theta_0) \\ \cos(m\theta_0) \\ \cos(m\theta_1) \\ \cos(m\theta_1) \\ \vdots \end{bmatrix} + \begin{bmatrix} -x_1 \\ x_0 \\ -x_3 \\ x_2 \\ \vdots \end{bmatrix} \odot \begin{bmatrix} \sin(m\theta_0) \\ \sin(m\theta_0) \\ \sin(m\theta_1) \\ \sin(m\theta_1) \\ \vdots \end{bmatrix}
$$

PyTorch伪代码：
```python
def apply_rope(x, cos, sin):
    # x: [batch, seq_len, num_heads, head_dim]
    x1, x2 = x[..., ::2], x[..., 1::2]  # 分离奇偶维度
    # 旋转操作
    x_rotated = torch.stack([-x2, x1], dim=-1).flatten(-2)
    return x * cos + x_rotated * sin
```

---

## 4. Sin/Cos vs RoPE 对比

| 特性 | Sinusoidal PE | RoPE |
|------|---------------|------|
| 编码方式 | 加法 ($x + PE$) | 乘法（旋转） |
| 位置信息注入点 | Embedding层 | Attention的Q, K |
| 相对位置 | 隐式（需模型学习） | 显式（内积直接反映） |
| 外推能力 | 较弱 | 较强 |
| 计算开销 | 低 | 中等 |
| 应用 | 原始Transformer, BERT | LLaMA, GPT-NeoX |

---

## 5. 总结

1. **Sinusoidal PE**：简单直观，通过三角函数的周期性编码位置，位置信息与语义信息相加混合
2. **RoPE**：更加优雅，通过旋转变换直接将相对位置关系编码到Attention计算中，具有更好的外推性和理论性质

现代大语言模型（如LLaMA系列）广泛采用RoPE，因为其在长序列处理和位置泛化方面表现更优。

---

## 参考文献

1. Vaswani et al. "Attention Is All You Need" (2017)
2. Su et al. "RoFormer: Enhanced Transformer with Rotary Position Embedding" (2021)
